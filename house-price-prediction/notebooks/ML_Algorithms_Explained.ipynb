{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9a9b226-f4fa-40a0-9792-88ec0781f9c4",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <h1 style=\"font-family: 'Times New Roman', Times, serif; font-size: 24px; font-weight: bold; color: #4a2c2a;\">\n",
    "        Comprehensive Guide to Machine Learning Models, Preprocessing, and Evaluation\n",
    "    </h1>\n",
    "</div>\n",
    "\n",
    "<div style=\"font-family: 'Times New Roman', Times, serif; line-height: 1.6; font-size: 16px;\">\n",
    "\n",
    "This document provides an overview of the specified machine learning models, cross-validation techniques, preprocessing tools, and evaluation metrics, including their mathematical operations and practical use cases. Each section explains what the component is, its mathematical foundation, and when/where it is used. All formulas are formatted in LaTeX for proper rendering in Jupyter Notebook.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Machine Learning Models\n",
    "<h2 style=\"font-family: 'Times New Roman', Times, serif; font-size: 20px; font-weight: bold; color: #4a2c2a;\">1. Machine Learning Models</h2>\n",
    "\n",
    "### LinearRegression\n",
    "<h3 style=\"font-family: 'Times New Roman', Times, serif; font-size: 18px; font-weight: bold; color: #4a2c2a;\">LinearRegression</h3>\n",
    "\n",
    "**What it is**: Linear Regression is a supervised machine learning algorithm used for predicting a continuous target variable based on one or more input features. It assumes a linear relationship between the input features and the target.\n",
    "\n",
    "**Mathematical Operation**:\n",
    "- The model fits a linear equation:  \n",
    "  $$ y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_n x_n + \\epsilon $$\n",
    "  where:\n",
    "  - $ y $: Target variable (predicted value)\n",
    "  - $ \\beta_0 $: Intercept\n",
    "  - $ \\beta_1, \\beta_2, \\dots, \\beta_n $: Coefficients for features $ x_1, x_2, \\dots, x_n $\n",
    "  - $ \\epsilon $: Error term\n",
    "- The goal is to minimize the **Mean Squared Error (MSE)**:\n",
    "  $$ \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 $$\n",
    "  where $ \\hat{y}_i $ is the predicted value.\n",
    "\n",
    "**When/Where Used**:\n",
    "- **When**: When the relationship between features and the target is approximately linear, and interpretability is important.\n",
    "- **Where**: Finance (e.g., predicting stock prices), economics (e.g., forecasting sales), and any domain with continuous outcomes and linear assumptions.\n",
    "- **Example**: Predicting house prices based on features like size, location, and number of bedrooms.\n",
    "\n",
    "---\n",
    "\n",
    "### RandomForestRegressor\n",
    "<h3 style=\"font-family: 'Times New Roman', Times, serif; font-size: 18px; font-weight: bold; color: #4a2c2a;\">RandomForestRegressor</h3>\n",
    "\n",
    "**What it is**: Random Forest Regressor is an ensemble learning method that combines multiple decision trees to predict a continuous target variable. It reduces overfitting by averaging predictions from many trees.\n",
    "\n",
    "**Mathematical Operation**:\n",
    "- Each decision tree splits the feature space based on feature values to minimize a loss function (e.g., MSE).\n",
    "- The final prediction is the **average** of predictions from all trees:\n",
    "  $$ \\hat{y} = \\frac{1}{T} \\sum_{t=1}^T \\hat{y}_t $$\n",
    "  where $ T $ is the number of trees, and $ \\hat{y}_t $ is the prediction from the $ t $-th tree.\n",
    "- Uses **bagging** (Bootstrap Aggregating) to create diverse trees by sampling data with replacement.\n",
    "\n",
    "**When/Where Used**:\n",
    "- **When**: When dealing with non-linear relationships, noisy data, or when feature interactions are complex.\n",
    "- **Where**: Applications like predicting customer spending, energy consumption, or medical outcomes.\n",
    "- **Example**: Predicting a patient’s blood pressure based on age, weight, and lifestyle factors.\n",
    "\n",
    "---\n",
    "\n",
    "### GradientBoostingRegressor\n",
    "<h3 style=\"font-family: 'Times New Roman', Times, serif; font-size: 18px; font-weight: bold; color: #4a2c2a;\">GradientBoostingRegressor</h3>\n",
    "\n",
    "**What it is**: Gradient Boosting Regressor is an ensemble method that builds decision trees sequentially, with each tree correcting the errors of the previous ones. It uses gradient descent to minimize a loss function.\n",
    "\n",
    "**Mathematical Operation**:\n",
    "- The model minimizes a loss function (e.g., MSE) by iteratively adding trees:\n",
    "  $$ F_m(x) = F_{m-1}(x) + \\eta \\cdot h_m(x) $$\n",
    "  where:\n",
    "  - $ F_m(x) $: Model prediction after $ m $ iterations\n",
    "  - $ h_m(x) $: Prediction from the $ m $-th tree\n",
    "  - $ \\eta $: Learning rate (controls step size)\n",
    "- The loss function (e.g., MSE) is optimized using gradient descent to find the direction of steepest descent.\n",
    "\n",
    "**When/Where Used**:\n",
    "- **When**: When high predictive accuracy is needed, and you can tolerate longer training times.\n",
    "- **Where**: Competitions (e.g., Kaggle), financial modeling, and time-series forecasting.\n",
    "- **Example**: Predicting insurance claim amounts based on policyholder data.\n",
    "\n",
    "---\n",
    "\n",
    "### StackingRegressor\n",
    "<h3 style=\"font-family: 'Times New Roman', Times, serif; font-size: 18px; font-weight: bold; color: #4a2c2a;\">StackingRegressor</h3>\n",
    "\n",
    "**What it is**: Stacking Regressor is an ensemble method that combines predictions from multiple base models (e.g., LinearRegression, RandomForestRegressor) using a meta-model (e.g., LinearRegression) to improve performance.\n",
    "\n",
    "**Mathematical Operation**:\n",
    "- Base models make predictions: $ \\hat{y}_1, \\hat{y}_2, \\dots, \\hat{y}_k $ for $ k $ base models.\n",
    "- The meta-model takes these predictions as input features and learns to predict the final output:\n",
    "  $$ \\hat{y}_{\\text{final}} = f(\\hat{y}_1, \\hat{y}_2, \\dots, \\hat{y}_k) $$\n",
    "  where $ f $ is the meta-model (e.g., a linear regression or another model).\n",
    "- Typically uses cross-validation to generate predictions for the meta-model to avoid overfitting.\n",
    "\n",
    "**When/Where Used**:\n",
    "- **When**: When you want to combine the strengths of multiple models to improve performance.\n",
    "- **Where**: Machine learning competitions, complex datasets with diverse patterns.\n",
    "- **Example**: Predicting sales by combining predictions from RandomForest and GradientBoosting models.\n",
    "\n",
    "---\n",
    "\n",
    "### CatBoostRegressor\n",
    "<h3 style=\"font-family: 'Times New Roman', Times, serif; font-size: 18px; font-weight: bold; color: #4a2c2a;\">CatBoostRegressor</h3>\n",
    "\n",
    "**What it is**: CatBoost Regressor is a gradient boosting algorithm optimized for categorical features, with built-in handling of categorical data and reduced overfitting.\n",
    "\n",
    "**Mathematical Operation**:\n",
    "- Similar to GradientBoostingRegressor, it builds trees sequentially to minimize a loss function (e.g., MSE).\n",
    "- Uses **ordered boosting** to reduce bias and **symmetric trees** for efficiency.\n",
    "- Automatically encodes categorical features using techniques like target encoding:\n",
    "  $$ x_{\\text{cat}} \\rightarrow \\text{encoded value based on target statistics} $$\n",
    "- Optimizes the loss function using gradient descent, similar to GradientBoosting.\n",
    "\n",
    "**When/Where Used**:\n",
    "- **When**: When the dataset has many categorical features or imbalanced data.\n",
    "- **Where**: Recommendation systems, fraud detection, and tabular data competitions.\n",
    "- **Example**: Predicting customer churn based on categorical features like subscription type.\n",
    "\n",
    "---\n",
    "\n",
    "### LGBMRegressor\n",
    "<h3 style=\"font-family: 'Times New Roman', Times, serif; font-size: 18px; font-weight: bold; color: #4a2c2a;\">LGBMRegressor</h3>\n",
    "\n",
    "**What it is**: LightGBM Regressor is a gradient boosting framework optimized for speed and scalability, particularly for large datasets.\n",
    "\n",
    "**Mathematical Operation**:\n",
    "- Similar to GradientBoosting, it builds trees sequentially to minimize a loss function.\n",
    "- Uses **histogram-based learning** to bin continuous features, reducing memory usage and speeding up training.\n",
    "- Employs **leaf-wise tree growth** (instead of level-wise) for better accuracy but risks overfitting.\n",
    "- Loss function optimization is similar to GradientBoosting.\n",
    "\n",
    "**When/Where Used**:\n",
    "- **When**: When working with large datasets or when training speed is critical.\n",
    "- **Where**: Real-time applications, large-scale tabular data tasks.\n",
    "- **Example**: Predicting delivery times in logistics based on route and weather data.\n",
    "\n",
    "---\n",
    "\n",
    "### XGBRegressor\n",
    "<h3 style=\"font-family: 'Times New Roman', Times, serif; font-size: 18px; font-weight: bold; color: #4a2c2a;\">XGBRegressor</h3>\n",
    "\n",
    "**What it is**: XGBoost Regressor is a highly optimized gradient boosting algorithm known for its speed, scalability, and performance.\n",
    "\n",
    "**Mathematical Operation**:\n",
    "- Builds trees sequentially to minimize a loss function, with regularization to prevent overfitting:\n",
    "  $$ \\text{Objective} = \\sum_{i=1}^n L(y_i, \\hat{y}_i) + \\sum_{m=1}^M \\Omega(h_m) $$\n",
    "  where:\n",
    "  - $ L $: Loss function (e.g., MSE)\n",
    "  - $ \\Omega $: Regularization term (e.g., L1 or L2 penalties on tree complexity)\n",
    "- Uses second-order gradient information (Hessian) for faster convergence.\n",
    "\n",
    "**When/Where Used**:\n",
    "- **When**: When you need high accuracy and can tune hyperparameters extensively.\n",
    "- **Where**: Kaggle competitions, financial modeling, and predictive maintenance.\n",
    "- **Example**: Predicting equipment failure probability in manufacturing.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Cross-Validation and Hyperparameter Tuning\n",
    "<h2 style=\"font-family: 'Times New Roman', Times, serif; font-size: 20px; font-weight: bold; color: #4a2c2a;\">2. Cross-Validation and Hyperparameter Tuning</h2>\n",
    "\n",
    "### KFold\n",
    "<h3 style=\"font-family: 'Times New Roman', Times, serif; font-size: 18px; font-weight: bold; color: #4a2c2a;\">KFold</h3>\n",
    "\n",
    "**What it is**: KFold is a cross-validation technique that splits the dataset into $ k $ subsets (folds) to evaluate model performance.\n",
    "\n",
    "**Mathematical Operation**:\n",
    "- The dataset is divided into $ k $ equal-sized folds.\n",
    "- For each fold $ i $:\n",
    "  - Train the model on $ k-1 $ folds.\n",
    "  - Test on the $ i $-th fold.\n",
    "- Compute the average performance metric (e.g., MSE) across all folds:\n",
    "  $$ \\text{CV Score} = \\frac{1}{k} \\sum_{i=1}^k \\text{Score}_i $$\n",
    "\n",
    "**When/Where Used**:\n",
    "- **When**: When you want to estimate model performance reliably and avoid overfitting to a single train-test split.\n",
    "- **Where**: Model evaluation, hyperparameter tuning, and comparing algorithms.\n",
    "- **Example**: Evaluating a RandomForestRegressor’s performance on a housing dataset.\n",
    "\n",
    "---\n",
    "\n",
    "### cross_val_score\n",
    "<h3 style=\"font-family: 'Times New Roman', Times, serif; font-size: 18px; font-weight: bold; color: #4a2c2a;\">cross_val_score</h3>\n",
    "\n",
    "**What it is**: A utility function in scikit-learn to perform KFold cross-validation and return performance scores for each fold.\n",
    "\n",
    "**Mathematical Operation**:\n",
    "- Implements KFold (or other cross-validation strategies) and computes a scoring metric (e.g., MSE, R²) for each fold.\n",
    "- Returns an array of scores, which can be averaged:\n",
    "  $$ \\text{Average Score} = \\frac{1}{k} \\sum_{i=1}^k \\text{Score}_i $$\n",
    "\n",
    "**When/Where Used**:\n",
    "- **When**: When you want a quick way to assess model performance across multiple folds.\n",
    "- **Where**: Model selection, performance benchmarking.\n",
    "- **Example**: Comparing LinearRegression and RandomForestRegressor using cross_val_score with MSE.\n",
    "\n",
    "---\n",
    "\n",
    "### RandomizedSearchCV\n",
    "<h3 style=\"font-family: 'Times New Roman', Times, serif; font-size: 18px; font-weight: bold; color: #4a2c2a;\">RandomizedSearchCV</h3>\n",
    "\n",
    "**What it is**: RandomizedSearchCV is a hyperparameter tuning method that randomly samples combinations of hyperparameters to find the best model configuration.\n",
    "\n",
    "**Mathematical Operation**:\n",
    "- Defines a parameter grid (e.g., ranges for learning rate, number of trees).\n",
    "- Randomly samples $ n $ combinations and evaluates each using cross-validation.\n",
    "- Selects the combination with the best average cross-validation score:\n",
    "  $$ \\text{Best Params} = \\arg\\max_{\\text{params}} \\left( \\frac{1}{k} \\sum_{i=1}^k \\text{Score}_i \\right) $$\n",
    "\n",
    "**When/Where Used**:\n",
    "- **When**: When the hyperparameter space is large, and exhaustive search (GridSearchCV) is too slow.\n",
    "- **Where**: Tuning complex models like RandomForest, XGBoost, or LightGBM.\n",
    "- **Example**: Tuning the number of trees and max depth for a RandomForestRegressor.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Preprocessing Tools\n",
    "<h2 style=\"font-family: 'Times New Roman', Times, serif; font-size: 20px; font-weight: bold; color: #4a2c2a;\">3. Preprocessing Tools</h2>\n",
    "\n",
    "### StandardScaler\n",
    "<h3 style=\"font-family: 'Times New Roman', Times, serif; font-size: 18px; font-weight: bold; color: #4a2c2a;\">StandardScaler</h3>\n",
    "\n",
    "**What it is**: StandardScaler standardizes features by removing the mean and scaling to unit variance, ensuring features are on the same scale.\n",
    "\n",
    "**Mathematical Operation**:\n",
    "- For each feature $ x $:\n",
    "  $$ x_{\\text{scaled}} = \\frac{x - \\mu}{\\sigma} $$\n",
    "  where:\n",
    "  - $ \\mu $: Mean of the feature\n",
    "  - $ \\sigma $: Standard deviation of the feature\n",
    "\n",
    "**When/Where Used**:\n",
    "- **When**: When features have different scales (e.g., age in years vs. income in dollars), and the model assumes standardized inputs (e.g., LinearRegression, SVM).\n",
    "- **Where**: Preprocessing for most machine learning models to improve convergence and performance.\n",
    "- **Example**: Standardizing house sizes and prices before training a LinearRegression model.\n",
    "\n",
    "---\n",
    "\n",
    "### OneHotEncoder\n",
    "<h3 style=\"font-family: 'Times New Roman', Times, serif; font-size: 18px; font-weight: bold; color: #4a2c2a;\">OneHotEncoder</h3>\n",
    "\n",
    "**What it is**: OneHotEncoder converts categorical variables into a binary (0/1) matrix representation, creating a new column for each category.\n",
    "\n",
    "**Mathematical Operation**:\n",
    "- For a categorical feature with $ k $ categories, creates $ k $ binary columns.\n",
    "- For each sample, the column corresponding to its category is set to 1, others to 0:\n",
    "  $$ \\text{Category}_i \\rightarrow [0, 0, \\dots, 1, \\dots, 0] $$\n",
    "\n",
    "**When/Where Used**:\n",
    "- **When**: When dealing with categorical features that need to be converted for numerical models.\n",
    "- **Where**: Preprocessing for models like LinearRegression, RandomForest, or neural networks.\n",
    "- **Example**: Encoding city names (e.g., \"New York,\" \"London\") into binary columns.\n",
    "\n",
    "---\n",
    "\n",
    "### SimpleImputer\n",
    "<h3 style=\"font-family: 'Times New Roman', Times, serif; font-size: 18px; font-weight: bold; color: #4a2c2a;\">SimpleImputer</h3>\n",
    "\n",
    "**What it is**: SimpleImputer fills missing values in a dataset using strategies like mean, median, or a constant value.\n",
    "\n",
    "**Mathematical Operation**:\n",
    "- For a feature with missing values:\n",
    "  - **Mean strategy**: Replace missing values with the mean:\n",
    "    $$ x_{\\text{missing}} = \\frac{1}{n} \\sum_{i=1}^n x_i $$\n",
    "  - **Median strategy**: Replace with the median.\n",
    "  - **Constant strategy**: Replace with a user-defined value.\n",
    "\n",
    "**When/Where Used**:\n",
    "- **When**: When the dataset has missing values that need to be handled before training.\n",
    "- **Where**: Preprocessing for any machine learning model, especially when missing data is common.\n",
    "- **Example**: Filling missing age values in a dataset with the mean age.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Evaluation Metrics\n",
    "<h2 style=\"font-family: 'Times New Roman', Times, serif; font-size: 20px; font-weight: bold; color: #4a2c2a;\">4. Evaluation Metrics</h2>\n",
    "\n",
    "### mean_absolute_error\n",
    "<h3 style=\"font-family: 'Times New Roman', Times, serif; font-size: 18px; font-weight: bold; color: #4a2c2a;\">mean_absolute_error</h3>\n",
    "\n",
    "**What it is**: Mean Absolute Error (MAE) measures the average absolute difference between predicted and actual values.\n",
    "\n",
    "**Mathematical Operation**:\n",
    "$$ \\text{MAE} = \\frac{1}{n} \\sum_{i=1}^n |y_i - \\hat{y}_i| $$\n",
    "where:\n",
    "- $ y_i $: Actual value\n",
    "- $ \\hat{y}_i $: Predicted value\n",
    "\n",
    "**When/Where Used**:\n",
    "- **When**: When you want a robust metric that is less sensitive to outliers than MSE.\n",
    "- **Where**: Regression tasks, especially when interpretability in the same units as the target is needed.\n",
    "- **Example**: Evaluating the error in predicting house prices.\n",
    "\n",
    "---\n",
    "\n",
    "### mean_squared_error\n",
    "<h3 style=\"font-family: 'Times New Roman', Times, serif; font-size: 18px; font-weight: bold; color: #4a2c2a;\">mean_squared_error</h3>\n",
    "\n",
    "**What it is**: Mean Squared Error (MSE) measures the average squared difference between predicted and actual values, emphasizing larger errors.\n",
    "\n",
    "**Mathematical Operation**:\n",
    "$$ \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 $$\n",
    "\n",
    "**When/Where Used**:\n",
    "- **When**: When you want to penalize larger errors more heavily (sensitive to outliers).\n",
    "- **Where**: Regression tasks, model training, and evaluation.\n",
    "- **Example**: Comparing model performance in predicting energy consumption.\n",
    "\n",
    "---\n",
    "\n",
    "### r2_score\n",
    "<h3 style=\"font-family: 'Times New Roman', Times, serif; font-size: 18px; font-weight: bold; color: #4a2c2a;\">r2_score</h3>\n",
    "\n",
    "**What it is**: R² Score (Coefficient of Determination) measures the proportion of variance in the target variable explained by the model.\n",
    "\n",
    "**Mathematical Operation**:\n",
    "$$ R^2 = 1 - \\frac{\\sum_{i=1}^n (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^n (y_i - \\bar{y})^2} $$\n",
    "where:\n",
    "- $ \\bar{y} $: Mean of actual values\n",
    "- $ \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 $: Sum of squared residuals\n",
    "- $ \\sum_{i=1}^n (y_i - \\bar{y})^2 $: Total sum of squares\n",
    "\n",
    "**When/Where Used**:\n",
    "- **When**: When you want to measure how well the model explains the variability of the target.\n",
    "- **Where**: Regression tasks to assess model fit.\n",
    "- **Example**: Evaluating how well a model predicts stock prices.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary Table\n",
    "<h2 style=\"font-family: 'Times New Roman', Times, serif; font-size: 20px; font-weight: bold; color: #4a2c2a;\">Summary Table</h2>\n",
    "\n",
    "| **Component**             | **Purpose**                     | **When to Use**                              | **Example Use Case**                     |\n",
    "|---------------------------|----------------------------------|----------------------------------------------|------------------------------------------|\n",
    "| LinearRegression          | Linear modeling                | Linear relationships, interpretability       | Predicting house prices                  |\n",
    "| RandomForestRegressor     | Ensemble tree-based            | Non-linear data, feature interactions        | Predicting customer spending             |\n",
    "| GradientBoostingRegressor | Sequential boosting            | High accuracy, complex patterns              | Predicting insurance claims              |\n",
    "| StackingRegressor         | Combining multiple models      | Leveraging diverse models                    | Combining models for sales prediction     |\n",
    "| CatBoostRegressor         | Gradient boosting for categoricals | Categorical-heavy datasets                | Predicting customer churn                |\n",
    "| LGBMRegressor             | Fast, scalable boosting         | Large datasets, speed-critical               | Predicting delivery times                |\n",
    "| XGBRegressor              | Optimized boosting             | High accuracy, extensive tuning              | Predicting equipment failure             |\n",
    "| KFold                     | Cross-validation               | Reliable performance estimation              | Evaluating model performance             |\n",
    "| cross_val_score           | Cross-validation scoring       | Quick model evaluation                      | Comparing model performance              |\n",
    "| RandomizedSearchCV        | Hyperparameter tuning          | Large hyperparameter space                  | Tuning RandomForest parameters           |\n",
    "| StandardScaler            | Feature scaling                | Different feature scales                    | Standardizing house sizes                |\n",
    "| OneHotEncoder             | Categorical encoding           | Categorical features                        | Encoding city names                     |\n",
    "| SimpleImputer             | Handling missing values        | Datasets with missing data                  | Filling missing ages                    |\n",
    "| mean_absolute_error       | Error measurement              | Robust error metric                         | Evaluating house price predictions       |\n",
    "| mean_squared_error        | Error measurement              | Penalizing large errors                     | Evaluating energy consumption predictions|\n",
    "| r2_score                  | Variance explained             | Assessing model fit                         | Evaluating stock price predictions       |\n",
    "\n",
    "---\n",
    "\n",
    "This Document provides a comprehensive overview of the specified machine learning components, their mathematical foundations, and practical applications.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfb74a3-4c37-469b-91b4-87a7ca59da64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
